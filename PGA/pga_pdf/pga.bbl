% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Fawaz:2019}
H.~I. Fawaz, G.~Forestier, J.~Weber, L.~Idoumghar, and P.~Muller, ``Deep
  learning for time series classification: a review,'' \emph{Data Mining and
  Knowledge Discovery}, vol.~33, no.~4, pp. 917--963, 2019.

\bibitem{Hu:2020}
J.~Hu, L.~Shen, S.~Albanie, G.~Sun, and E.~Wu, ``Squeeze-and-excitation
  networks,'' \emph{{IEEE} Transactions on Pattern Analysis and Machine
  Intelligence}, vol.~42, no.~8, pp. 2011--2023, 2020.

\bibitem{He:2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition}, 2016, pp. 770--778.

\bibitem{Szegedy:2015}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~E. Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
  \emph{{IEEE} Conference on Computer Vision and Pattern Recognition}, 2015,
  pp. 1--9.

\bibitem{UCRArchive}
W.~V. Anthony~Bagnall, Jason~Lines and E.~Keogh, ``The uea \& ucr time series
  classification repository,'' 2018, uRL:
  http://www.timeseriesclassification.com/.

\bibitem{Wang:2017}
Z.~Wang, W.~Yan, and T.~Oates, ``Time series classification from scratch with
  deep neural networks: {A} strong baseline,'' in \emph{International Joint
  Conference on Neural Networks}, 2017, pp. 1578--1585.

\bibitem{Fawaz:2020}
H.~I. Fawaz, B.~Lucas, G.~Forestier, C.~Pelletier, D.~F. Schmidt, J.~Weber,
  G.~I. Webb, L.~Idoumghar, P.~Muller, and F.~Petitjean, ``Inceptiontime:
  Finding alexnet for time series classification,'' \emph{Data Mining and
  Knowledge Discovery}, vol.~34, no.~6, pp. 1936--1962, 2020.

\bibitem{LeCun:1998}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' \emph{Proceedings of the IEEE}, vol.~86,
  no.~11, pp. 2278--2324, 1998.

\bibitem{CIFAR10}
A.~Krizhevsky, ``Learning multiple layers of features from tiny images,'' 2009,
  uRL: https://www.cs.utoronto.ca/$\sim$kriz/cifar.html.

\bibitem{Deng:2009}
J.~Deng, W.~Dong, R.~Socher, L.~Li, K.~Li, and L.~Fei{-}Fei, ``Imagenet: {A}
  large-scale hierarchical image database,'' in \emph{{IEEE} Conference on
  Computer Vision and Pattern Recognition}, 2009, pp. 248--255.

\bibitem{Lines:2018}
J.~Lines, S.~Taylor, and A.~J. Bagnall, ``Time series classification with
  {HIVE-COTE:} the hierarchical vote collective of transformation-based
  ensembles,'' \emph{{ACM} Transactions on Knowledge Discovery from Data},
  vol.~12, no.~5, pp. 52:1--52:35, 2018.

\bibitem{Dempster:2020}
A.~Dempster, F.~Petitjean, and G.~I. Webb, ``{ROCKET:} exceptionally fast and
  accurate time series classification using random convolutional kernels,''
  \emph{Data Mining and Knowledge Discovery}, vol.~34, no.~5, pp. 1454--1495,
  2020.

\bibitem{Ioffe:2015}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in \emph{International
  Conference on Machine Learning}, 2015, pp. 448--456.

\bibitem{Srivastava:2014}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: a simple way to prevent neural networks from overfitting,''
  \emph{The Journal of Machine Learning Research}, vol.~15, no.~1, pp.
  1929--1958, 2014.

\bibitem{Szegedy:2016}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna, ``Rethinking the
  inception architecture for computer vision,'' in \emph{IEEE Conference on
  Computer Vision and Pattern Recognition}, 2016, pp. 2818--2826.

\bibitem{Wan:2017}
Y.~Wan and Y.~Si, ``A formal approach to chart patterns classification in
  financial time series,'' \emph{Information Sciences}, vol. 411, pp. 151--175,
  2017.

\bibitem{Hinton:2015}
G.~Hinton, O.~Vinyals, and J.~Dean, ``Distilling the knowledge in a neural
  network,'' \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{Bagnall:2017}
A.~J. Bagnall, J.~Lines, A.~Bostrom, J.~Large, and E.~J. Keogh, ``The great
  time series classification bake off: a review and experimental evaluation of
  recent algorithmic advances,'' \emph{Data Mining and Knowledge Discovery},
  vol.~31, no.~3, pp. 606--660, 2017.

\bibitem{Rakthanmanon:2012}
T.~Rakthanmanon, B.~J.~L. Campana, A.~Mueen, G.~E. A. P.~A. Batista, M.~B.
  Westover, Q.~Zhu, J.~Zakaria, and E.~J. Keogh, ``Searching and mining
  trillions of time series subsequences under dynamic time warping,'' in
  \emph{International Conference on Knowledge Discovery and Data Mining}, 2012,
  pp. 262--270.

\bibitem{Sakurai:2007}
Y.~Sakurai, C.~Faloutsos, and M.~Yamamuro, ``Stream monitoring under the time
  warping distance,'' in \emph{International Conference on Data Engineering},
  2007, pp. 1046--1055.

\bibitem{Gong:2018}
X.~Gong, S.~Fong, and Y.~Si, ``Fast multi-subsequence monitoring on streaming
  time-series based on forward-propagation,'' \emph{Information Sciences}, vol.
  450, pp. 73--88, 2018.

\bibitem{Baydogan:2013}
M.~G. Baydogan, G.~C. Runger, and E.~Tuv, ``A bag-of-features framework to
  classify time series,'' \emph{{IEEE} Transactions on Pattern Analysis and
  Machine Intelligence}, vol.~35, no.~11, pp. 2796--2802, 2013.

\bibitem{Kate:2016}
R.~J. Kate, ``Using dynamic time warping distances as features for improved
  time series classification,'' \emph{Data Mining and Knowledge Discovery},
  vol.~30, no.~2, pp. 283--312, 2016.

\bibitem{Bagnall:2016}
A.~J. Bagnall, J.~Lines, J.~Hills, and A.~Bostrom, ``Time-series classification
  with {COTE:} the collective of transformation-based ensembles,'' in
  \emph{International Conference on Data Engineering}, 2016, pp. 1548--1549.

\bibitem{Cui:2016}
Z.~Cui, W.~Chen, and Y.~Chen, ``Multi-scale convolutional neural networks for
  time series classification,'' \emph{arXiv preprint arXiv:1603.06995}, 2016.

\bibitem{Karimi-Bidhendi:2018}
S.~Karimi{-}Bidhendi, F.~Munshi, and A.~Munshi, ``Scalable classification of
  univariate and multivariate time series,'' in \emph{International Conference
  on Big Data}, 2018, pp. 1598--1605.

\bibitem{Szegedy:2017}
C.~Szegedy, S.~Ioffe, V.~Vanhoucke, and A.~A. Alemi, ``Inception-v4,
  inception-resnet and the impact of residual connections on learning,'' in
  \emph{{AAAI} Conference on Artificial Intelligence}, 2017, pp. 4278--4284.

\bibitem{Chen:2021}
W.~Chen and K.~Shi, ``Multi-scale attention convolutional neural network for
  time series classification,'' \emph{Neural Networks}, vol. 136, pp. 126--140,
  2021.

\bibitem{Gou:2021}
J.~Gou, B.~Yu, S.~J. Maybank, and D.~Tao, ``Knowledge distillation: {A}
  survey,'' \emph{International Journal of Computer Vision}, vol. 129, no.~6,
  pp. 1789--1819, 2021.

\bibitem{Romero:2014}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio,
  ``Fitnets: Hints for thin deep nets,'' in \emph{International Conference on
  Learning Representations}, 2015.

\bibitem{Yim:2017}
J.~Yim, D.~Joo, J.~Bae, and J.~Kim, ``A gift from knowledge distillation: Fast
  optimization, network minimization and transfer learning,'' in \emph{{IEEE}
  Conference on Computer Vision and Pattern Recognition}, 2017, pp. 7130--7138.

\bibitem{Svitov:2020}
D.~Svitov and S.~Alyamkin, ``Margindistillation: Distillation for margin-based
  softmax,'' \emph{arXiv preprint arXiv:2003.02586}, 2020.

\bibitem{Oki:2020}
H.~Oki, M.~Abe, J.~Miyao, and T.~Kurita, ``Triplet loss for knowledge
  distillation,'' in \emph{International Joint Conference on Neural Networks},
  2020, pp. 1--7.

\bibitem{Cho:2019}
J.~H. Cho and B.~Hariharan, ``On the efficacy of knowledge distillation,'' in
  \emph{International Conference on Computer Vision}, 2019, pp. 4793--4801.

\bibitem{Mirzadeh:2020}
S.~Mirzadeh, M.~Farajtabar, A.~Li, N.~Levine, A.~Matsukawa, and H.~Ghasemzadeh,
  ``Improved knowledge distillation via teacher assistant,'' in \emph{{AAAI}
  Conference on Artificial Intelligence}, 2020, pp. 5191--5198.

\bibitem{Friedman:1940}
M.~Friedman, ``A comparison of alternative tests of significance for the
  problem of m rankings,'' \emph{The Annals of Mathematical Statistics},
  vol.~11, no.~1, pp. 86--92, 1940.

\bibitem{Benavoli:2016}
A.~Benavoli, G.~Corani, and F.~Mangili, ``Should we really use post-hoc tests
  based on mean-ranks?'' \emph{The Journal of Machine Learning Research},
  vol.~17, no.~1, pp. 152--161, 2016.

\bibitem{Garcia:2008}
S.~Garcia and F.~Herrera, ``An extension on" statistical comparisons of
  classifiers over multiple data sets" for all pairwise comparisons.''
  \emph{Journal of machine learning research}, vol.~9, no.~12, 2008.

\bibitem{Demsar:2006}
J.~Dem{\v{s}}ar, ``Statistical comparisons of classifiers over multiple data
  sets,'' \emph{The Journal of Machine Learning Research}, vol.~7, pp. 1--30,
  2006.

\end{thebibliography}
