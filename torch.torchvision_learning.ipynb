{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19538124",
   "metadata": {},
   "source": [
    "tochvision 主要处理图像数据，包含一些常用的数据集、模型、转换函数等。torchvision独立于PyTorch，需要专门安装，torchvision主要包含以下四部分：\n",
    "\n",
    "torchvision.models: 提供深度学习中各种经典的网络结构、预训练好的模型，如：Alex-Net、VGG、ResNet、Inception等\n",
    "\n",
    "torchvision.datasets：提供常用的数据集，设计上继承 torch.utils.data.Dataset，主要包括：MNIST、CIFAR10/100、ImageNet、COCO等\n",
    "\n",
    "torchvision.transforms：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作\n",
    "\n",
    "torchvision.utils：工具类，如保存张量作为图像到磁盘，给一个小批量创建一个图像网格\n",
    "\n",
    "torchvision要注意与pytorch版本和Cuda相匹配。要查询pytorch和torchvision的版本，可以使用下面语句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9155ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "0.23.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d63751",
   "metadata": {},
   "source": [
    "通过 pretrined=True 可以加载预训练模型。pretrained 默认值是 False，不赋值和赋值 False 效果一样\n",
    "\n",
    "选择了 pretrained 为 True 的话，点击运行会执行下载预训练参数的操作，不加载预训练模型没有这步操作，可以直接使用网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ec025f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "fc.weight torch.Size([1000, 512])\n",
      "fc.bias torch.Size([1000])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary as sumy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "vgg16 = models.vgg16(weights='IMAGENET1K_V1')\n",
    "alexnet = models.alexnet(weights='IMAGENET1K_V1')\n",
    "squeezenet = models.squeezenet1_0(weights='IMAGENET1K_V1')\n",
    "densenet = models.densenet161(weights='IMAGENET1K_V1')\n",
    "resnet152 = models.resnet152()\n",
    "# 可以使用该方法打印加载的预训练模型的参数\n",
    "for name, param in resnet18.named_parameters():\n",
    "    print(name, param.shape)\n",
    "vgg16 = vgg16.to(device)\n",
    "sumy(vgg16, input_size=(3, 224, 224), device=str(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e95fe",
   "metadata": {},
   "source": [
    "vgg16.state_dict() 前面方法得到模型的参数\n",
    "\n",
    "加载本地的模型参数使用方法 vgg16.load_state_dict()\n",
    "\n",
    "需要结合加载文件转化为字典的函数 torch.load_dict() 一起使用，也就是 vgg16.load_state_dict(torch.load_dict(xxxx.pth))\n",
    "\n",
    "保存模型的参数需要得到模型和的字典，然后使用 torch.save() 函数来保存，torch.save(vgg16.state_dict(), \"xxxx.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b35e645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_state_dict = vgg16.state_dict()\n",
    "resnet18_state_dict = resnet18.state_dict()\n",
    "\n",
    "torch.save(vgg16_state_dict, \"./vgg16_state_dict.pth\")\n",
    "torch.save(resnet18_state_dict, \"./resnet18_state_dict.pth\")\n",
    "\n",
    "vgg16.load_state_dict(torch.load(\"./vgg16_state_dict.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fea2eb",
   "metadata": {},
   "source": [
    "但是正常的使用过程中可能会修改模型的层数，想要接着使用预训练的参数就会出现无法加载的问题，所以必须在 pth 文件里面删掉或者加上相应的修改的层\n",
    "\n",
    "加入现在 resnet18 的分类任务变为了分类为 10 类，就需要对最后的全连接层进行修改\n",
    "\n",
    "层的名字和 shape 都是匹配的话，成功的加载，名字不匹配但是形状匹配的话，strict=False 会忽略错误，名字匹配但是形状不匹配，都会报错\n",
    "\n",
    "pretrained_dict = {k : v for k, v in pretrained_dict.items() if k in model_dict} 这个写法很显然只会比较键是不是完全一样，即使修改了形状也不会关心，所以需要加上判断，v.shape 和 model_dict[k].shape 要是一样的才行，这样的话，只会从预训练模型里面加载确定都没有修改的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af03667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([1000])\n",
      "Missing keys: ['fc.weight', 'fc.bias']\n",
      "Unexpected keys: []\n",
      "tensor(9.4485e-06, grad_fn=<MeanBackward0>) tensor(0.0254, grad_fn=<StdBackward0>)\n",
      "tensor(5.8502e-08, grad_fn=<MeanBackward0>) tensor(0.0695, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = models.resnet18(weights=None)\n",
    "\n",
    "model.fc = nn.Linear(512, 10)\n",
    "pretrained_dict = resnet18.state_dict()\n",
    "model_dict = model.state_dict()\n",
    "print(pretrained_dict.keys() == model_dict.keys())\n",
    "print(pretrained_dict['fc.bias'].size())\n",
    "pretrained_dict = {k : v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "# print(pretrained_dict['fc.bias']) 这句话会报错因为修改了全连接层，这个参数没有被加载\n",
    "# res = model.load_state_dict(torch.load('./resnet18_state_dict.pth'), strict=False) 报错，因为无法对应，原本的预训练模型的全连接层的大小为 [512, 1000]，现在被我修改为了 [512, 10]\n",
    "res = model.load_state_dict(pretrained_dict, strict=False) # 完美的加载，验证了自己的想法，现在删掉了全连接层的参数，其他的参数的预训练的参数了\n",
    "\n",
    "print(\"Missing keys:\", res.missing_keys) # 模型中存在，但是在预训练参数里面找不到\n",
    "print(\"Unexpected keys:\", res.unexpected_keys) # 预训练参数中存在模型中没定义，这个不会遇到，因为我强制的比对了键值对，模型中没定义的一定会被删掉的\n",
    "\n",
    "print(model.fc.weight.mean(), model.fc.weight.std())  # 随机分布\n",
    "print(resnet18.fc.weight.mean(), resnet18.fc.weight.std())  # 不同分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a32a2",
   "metadata": {},
   "source": [
    "上面简单的介绍了模型的加载和报错，和加载预训练参数的方法等，这个是 torchvision.models 库函数里面常见的操作了，总结下列函数：\n",
    "\n",
    "models.model() 加载名字为 model 的模型，一般的参数为 weights，代表加载预训练参数，不想加载的话初始化为 weights=None\n",
    "\n",
    "model.named_parameters() 返回键值对，代表层的名字和对应的 shape\n",
    "\n",
    "summary() 能将一张指定大小的图片走一遍网络的流程，看一下具体经过一层之后变化为多大的尺寸\n",
    "\n",
    "torch.save() 保存模型\n",
    "\n",
    "torch.load_state_dict() 加载模型\n",
    "\n",
    "torch.load() 加载文件转化为字典\n",
    "\n",
    "model.state_dict() 得到模型的参数，通过 [键] 能得到对应的值，也就是这个名字的层的大小，也可以直接 name_parameters() 输出键值对，是一样的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f521779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBILee6mENvDJNKwJCRqWY4GTwPYE0sttPAEM0MkYfO0upG7HBxnrUVFXdJ1a+0PVINT02cwXlu26KUKDtOMdCCDwTXVw/FvxerBrq9gvyGDD7ZbJJjC7cDjgHgnHUjPrnpfiBrpPw60uz1SOwbXtTcXkiW8aL9mg/5Z7TGcYYANhi2d2eCBjyKiu8+GHhy01LVLvXtY8n+xNDj+1XSy8iRhkohXvkg8Y5xjB6VzHiTXbnxJr11qlzgNK3yIudsaDhVUEnAA7dKyqK6lfiBrMXgVPCFutrBpwLGV44j5s+5i2HYkg8nsBwB6Vy1Ff/Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4AWJkZMANmHBLMQw+SRaIa1eePPPB68IOCAdOQv25x4GBgeHZKqjwCZEZEBYjGHQ8m/TM/fjfw3///v16+u9fSbAgI1QnWKFS8vRsBob3W9ap3NQGC0B1wlSC6BW/F6uCaEZGzODT0WZhvQPRiKmT+e/dSIhGRgzJ8s9/a6FymJLcf8u5YZLodgpN+PLvG9RGDDv//c2F6WNkhAYfTO30n6/YYGwGNJ2Aqe/+64vQiGonW4HTrl9IGlG84vv37xwkjShesT/49wUzLknuv38zkeWQ7ay6hWwdmI1Q+uXv3+2BCC4oNhG8L3/nQ6MKIQbWj50AAET1Q0O22Fn9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "mnist = datasets.MNIST('data/', download=True, train=False, transform=transform)\n",
    "# root 数据集的存放路径，没有的话，会自动存放到该文件夹\n",
    "# train 是否加载训练集，否则的话就是测试集\n",
    "# transform torchvision.transforms 包含对图像进行变换的函数，ToTensor()、Normalize()\n",
    "# target_transform torchvision.transforms 对标签变换的函数，一般不使用\n",
    "# download 如果本地没有数据集，是否从网上下载\n",
    "\n",
    "img = mnist[0][0]\n",
    "label = mnist[0][1]\n",
    "to_img = T.ToPILImage()\n",
    "display(to_img(img), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2da528",
   "metadata": {},
   "source": [
    "当第一次调用 datasets.MNIST(download=True) 时，它会自动执行以下步骤检查 raw 文件夹如果没有 .gz 文件，就从官网（yann.lecun.com）下载，自动解压 .gz 文件\n",
    "\n",
    "存入 data/MNIST/processed/，解析二进制数据为 Tensor，把像素值解析成 torch.Tensor 格式，把标签解析成 torch.LongTensor，缓存结果，将解析好的数据保存为两个 .pt 文件\n",
    "\n",
    "training.pt  → 包含 (train_images, train_labels)，参考上面是输出 dataset 是一个存储文件，保存了 Tensor 类型的文件和对应的标签\n",
    "test.pt      → 包含 (test_images, test_labels)\n",
    "\n",
    "因为 datasets.MNIST 继承自 torch.utils.data.Dataset，实现了 \\_\\_gititem\\_\\_ 和 \\_\\_len\\_\\_ 两个函数，可以直接 train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "不需要进行其他任何多余的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5addbad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "0 torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(mnist, batch_size=64, shuffle=True)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "batch_img, batch_label = next(train_iter)\n",
    "\n",
    "print(batch_img.shape, batch_label.shape)\n",
    "for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "    print(batch_idx, imgs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d7c2d",
   "metadata": {},
   "source": [
    "PyTorch 里所有能被 DataLoader 使用的数据集,都必须继承 torch.utils.data.Dataset，并实现这两个方法：\\_\\_getitem\\_\\_()：告诉 PyTorch 怎么取一条数据；\\_\\_len\\_\\_()：告诉 PyTorch 数据集多大\n",
    "\n",
    "dataset 对象，加载的数据集，要是自己创建的数据集，需要手动实现上面说的类和类里面必须实现的方法，batch_size 每个批次的样本数量，shuffle 是否打乱数据，num_workers 加载数据的线程数，drop_last 最后一个 batch 不足 batch_size 大小，是否丢弃\n",
    "\n",
    "train_data.data 所有图片 torch.Tensor (形状 [60000, 28, 28])\n",
    "\n",
    "train_data.targets 所有标签 torch.Tensor (形状 [60000])\n",
    "\n",
    "image, label = train_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5850cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AugMix',\n",
       " 'AutoAugment',\n",
       " 'AutoAugmentPolicy',\n",
       " 'CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'ElasticTransform',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'InterpolationMode',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandAugment',\n",
       " 'RandomAdjustSharpness',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomAutocontrast',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomEqualize',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomInvert',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomPosterize',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSolarize',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " 'TrivialAugmentWide',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional_pil',\n",
       " '_functional_tensor',\n",
       " '_presets',\n",
       " 'autoaugment',\n",
       " 'functional',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "# 包含了常见的图像处理的函数\n",
    "\n",
    "dir(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae4fc2",
   "metadata": {},
   "source": [
    "Compose()：用来管理所有的 transforms 操作\n",
    "\n",
    "ToTensor()：把图片数据转换成张量并转化范围在 [0,1] 区间内\n",
    "\n",
    "Normalize(mean, std)：归一化\n",
    "\n",
    "Resize(size)：输入的 PIL 图像调整为指定的大小，参数可以为 int 或 int 元组\n",
    "\n",
    "CenterCrop(size)：将给定的 PIL Image 进行中心切割，得到指定 size 的 tuple\n",
    "\n",
    "RandomCrop(size, padding=0)：随机中心点切割\n",
    "\n",
    "RandomHorizontalFlip(size, interpolation=2)：将给定的 PIL Image 随机切割，再 resize\n",
    "\n",
    "RandomHorizontalFlip()：随机水平翻转给定的 PIL Image\n",
    "\n",
    "RandomVerticalFlip()：随机垂直翻转给定的 PIL Image\n",
    "\n",
    "ToPILImage()：将 Tensor 或 numpy.ndarray 转换为 PIL Image\n",
    "\n",
    "FiveCrop(size)：将给定的 PIL 图像裁剪成 4 个角落区域和中心区域\n",
    "\n",
    "Pad(padding, fill=0, padding_mode=‘constant’)：对 PIL 边缘进行填充\n",
    "\n",
    "RandomAffine(degrees, translate=None, scale=None)：保持中心不变的图片进行随机仿射变化\n",
    "\n",
    "RandomApply(transforms, p=0.5)：随机选取变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfbca8",
   "metadata": {},
   "source": [
    "下面自己构建一个 MNIST 类，学习一下一般方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|##########| 10/10 [00:00<00:00, 501.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, root, transform=None, preload=False):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.preload = preload\n",
    "        self.data = []\n",
    "        classes = sorted(os.listdir(root))\n",
    "        for label, cls in enumerate(tqdm(classes, desc=\"processing\", ascii=True)):\n",
    "            folder = os.path.join(root, cls)\n",
    "            for img in os.listdir(folder):\n",
    "                path = os.path.join(folder, img)\n",
    "                self.samples.append((path, label))\n",
    "        if self.preload:\n",
    "            for path, label in self.samples:\n",
    "                img = Image.open(path).convert('L')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                self.data.append((img, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.preload:\n",
    "            return self.data[index]\n",
    "        path, label = self.samples[index]\n",
    "        img = Image.open(path).convert('L')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "transform = T.ToTensor()\n",
    "dataset = MNIST('./mnist_test_torch', transform=transform, preload=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for imgs, labels in loader: \n",
    "    # 执行这句话的时候 __getitem__ 才会执行，相当于进行下面的操作\n",
    "    # batch_data = []，for idx in [i1, i2, ..., i32]:，sample = dataset.__getitem__(idx)，batch_data.append(sample)\n",
    "    # 每张图片都要执行一边 getitem，设计图像的解码，打开，转换等操作，本质上是很慢的，它存在的意义就是有人需要对图像进行增强，一张图片可能被改编成很多种的可能，直接存储静态的文件实现不了这一点\n",
    "    print(imgs.shape, labels.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202510-learning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
