{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddfd798",
   "metadata": {},
   "source": [
    "##### 1. torchvision 概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19538124",
   "metadata": {},
   "source": [
    "> torchvision 是 torch 官方提供的视觉任务工具包，包含用于图像与视频数据的常用数据集加载、预训练模型、图像变换（transform）以及可视化工具。torchvision 独立于 torch，需要专门安装，torchvision 主要包含以下四部分：\n",
    "\n",
    "- 1）torchvision.models: 模型包括 分类、检测、分割、视频理解四大类：\n",
    "\n",
    "- - 1.1）在分类方面提供深度学习中各种经典的网络结构，如：Alex-Net、VGG、ResNet、Inception 等\n",
    "\n",
    "- - 1.2）检测方面提供了常见的检测用的网络结构，FasterRCNN、RetinaNet、MaskRCNN 等\n",
    "\n",
    "- - 1.3）分割方面提供了常见的分割用的网络结构，FCN, DeepLabV3, LRASPP 等\n",
    "\n",
    "- - 1.4）视频理解方面不打算涉及，暂时不做学习\n",
    "\n",
    "- 2）torchvision.datasets：提供常用的数据集，设计上继承 torch.utils.data.Dataset，主要包含分类、检测、分割、视频理解四大类常见数据集：\n",
    "\n",
    "- - 2.1）分类方面主要包括：MNIST、FashionMNIST、CIFAR10/100、ImageNet、SVHN、Places365 等\n",
    "\n",
    "- - 2.2）检测、分割方面主要包含：VOC、COCO、Cityscapes、CelebA、Caltech101/256 等\n",
    "\n",
    "- - 2.3）视频理解方面不打算涉及，暂时不做学习\n",
    "\n",
    "- 3）torchvision.transforms：提供常用的数据预处理操作，主要包括对 Tensor 及 PIL Image 对象的操作，新版的 transforms 有两套接口：\n",
    "\n",
    "- - 3.1）传统版 (transforms)：操作 PIL 图像或 Tensor\n",
    "\n",
    "- - 3.2）新版 (transforms.v2)：统一操作 tv_tensors（带标签的张量类型），支持批处理和自动类型推断，更安全且更快\n",
    "\n",
    "- 4）torchvision.utils：工具类，如保存张量作为图像到磁盘，给一个小批量创建一个图像网格，该模块使用不太多，等用到会进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e9155ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "0.23.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# 与 torch 主体完全兼容，但安装时需要与 torch 版本和 CUDA 版本匹配，要查询 torch 和 torchvision 的版本，可以使用下面语句\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ad7b6",
   "metadata": {},
   "source": [
    "##### 2. torchvision.models 的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d63751",
   "metadata": {},
   "source": [
    "> 1）torchvision.models: 模型包括 分类、检测、分割、视频理解四大类：\n",
    "\n",
    "- - 1.1）在分类方面提供深度学习中各种经典的网络结构，如：Alex-Net、VGG、ResNet、Inception 等\n",
    "\n",
    "- - 1.2）检测方面提供了常见的检测用的网络结构，FasterRCNN、RetinaNet、MaskRCNN 等\n",
    "\n",
    "- - 1.3）分割方面提供了常见的分割用的网络结构，FCN, DeepLabV3, LRASPP 等\n",
    "\n",
    "- - 1.4）视频理解方面不打算涉及，暂时不做学习\n",
    "\n",
    "> torchvision.models 里面包含了很多的常见的模型，可以直接的加载使用，也可以直接加载预训练模型在自己想进行的任务上微调使用\n",
    "\n",
    "- 1）通过 pretrined=True 可以加载预训练模型，pretrained 默认值是 False，不赋值和赋值 False 效果一样，选择了 pretrained 为 True 的话，点击运行会执行下载预训练参数的操作，不加载预训练模型没有这步操作，可以直接只使用网络架构\n",
    "\n",
    "- 2）预训练模型使用上面的参数进行下载的时候现在会出现警告，主要是因为 torch 进行了更新，现在调用预训练模型的方法是在参数 weights 上指定调用的预训练模型的名字，参考下面的写法\n",
    "\n",
    "- 3）直接输出当前的模型或者遍历输出其参数矩阵的名字和大小能检查当前的网络的形状，可以使用 torchsummary 库函数下的 summary 包来检查网络的结构，更加的直观"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ec025f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture (show first 10 layers):\n",
      "Layer 0:  conv1.weight torch.Size([64, 3, 7, 7])\n",
      "Layer 1:  bn1.weight torch.Size([64])\n",
      "Layer 2:  bn1.bias torch.Size([64])\n",
      "Layer 3:  layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "Layer 4:  layer1.0.bn1.weight torch.Size([64])\n",
      "Layer 5:  layer1.0.bn1.bias torch.Size([64])\n",
      "Layer 6:  layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "Layer 7:  layer1.0.bn2.weight torch.Size([64])\n",
      "Layer 8:  layer1.0.bn2.bias torch.Size([64])\n",
      "Layer 9:  layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary as sumy\n",
    "\n",
    "# torch.device 是一个类，表示一个设备或者对象\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resnet18 = models.resnet18(pretrained=True) # 旧版本写法会出现警告，该方法将被淘汰，不建议使用\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "squeezenet = models.squeezenet1_0(weights=models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "densenet = models.densenet161(weights=models.DenseNet161_Weights.IMAGENET1K_V1)\n",
    "resnet152 = models.resnet152(weights=None)\n",
    "\n",
    "# 可以使用该方法打印加载的预训练模型的参数\n",
    "# 第一次接触 model.named_parameters()，他是 torch.nn.Module 类的一个方法。\n",
    "# 它返回一个可迭代对象（generator），其中每个元素都是一个二元组 (name, parameter)，下面输出前十层的名字和参数矩阵的 shape 展示\n",
    "print(\"Model architecture (show first 10 layers):\")\n",
    "for i, (name, param) in enumerate(resnet18.named_parameters()):\n",
    "    print(f\"Layer {i}: \", name, param.shape)\n",
    "    if i == 9: break\n",
    "\n",
    "# .to() 是 torch.Tensor 类的一个实例方法\n",
    "# 当整个模型 .to() 的时候，此时 .to() 是 torch.nn.Module 类 的一个方法，它会自动遍历所有子层的参数和缓冲区（buffers），把它们都放到目标设备上\n",
    "vgg16 = vgg16.to(device)\n",
    "\n",
    "# summary 函数的参数：model：要查看的模型对象、input_size：输入的张量尺寸、device：指定设备上生成 summary，其他参数不重要用到可查\n",
    "# 输出的 Output Shape 的第一个维度为 -1 代表是任意 batch_size 大小\n",
    "sumy(vgg16, input_size=(3, 224, 224), device=str(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e95fe",
   "metadata": {},
   "source": [
    "> 下面介绍常见的模型加载和保存的方法\n",
    "\n",
    "- 1）vgg16.state_dict() 得到模型的参数\n",
    "\n",
    "- 2）加载本地的模型参数使用方法 vgg16.load_state_dict()\n",
    "\n",
    "- 3）需要结合加载文件转化为字典的函数 torch.load_dict() 一起使用，也就是 vgg16.load_state_dict(torch.load_dict(xxxx.pth))\n",
    "\n",
    "- 4）保存模型的参数需要得到模型和的字典，然后使用 torch.save() 函数来保存，torch.save(vgg16.state_dict(), \"xxxx.pth\")\n",
    "\n",
    "- 5）torch.save() 可以保存训练过程中的其他参数，方便中断之后继续训练，保存的时候写成键值对的形式即可，这样的保存形式加载的时候需要指定对应的键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b35e645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_state_dict = vgg16.state_dict()\n",
    "resnet18_state_dict = resnet18.state_dict()\n",
    "\n",
    "torch.save(vgg16_state_dict, \"./checkpoints/vgg16_state_dict.pth\")\n",
    "torch.save(resnet18_state_dict, \"./checkpoints/resnet18_state_dict.pth\")\n",
    "\n",
    "vgg16.load_state_dict(torch.load(\"./checkpoints/vgg16_state_dict.pth\"))\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": None,\n",
    "    \"model_state_dict\": vgg16_state_dict,\n",
    "    \"optimizer_state_dict\": None, # 假设训练使用的是带动量的 SGD 优化器，优化器参数保存了之前的动量，所以要从断点继续训练，这个参数是必须保存的\n",
    "    \"lr\": None, # 断点恢复训练必须保存\n",
    "    \"loss\": None\n",
    "}, \"./checkpoints/vgg16.pth\")\n",
    "\n",
    "vgg16.load_state_dict(torch.load(\"./checkpoints/vgg16.pth\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fea2eb",
   "metadata": {},
   "source": [
    "- 1）但是正常的使用过程中可能会修改模型的层数，想要接着使用预训练的参数就会出现无法加载的问题，所以必须在 pth 文件里面删掉或者加上相应的修改的层\n",
    "\n",
    "- 2）加入现在 resnet18 的分类任务变为了分类为 10 类，就需要对最后的全连接层进行修改，层的名字和 shape 都是匹配的话，成功的加载，名字不匹配但是形状匹配的话，strict=False 会忽略错误，名字匹配但是形状不匹配，就会报错\n",
    "\n",
    "- 3）pretrained_state_dict = {k : v for k, v in pretrained_state_dict.items() if k in model_state_dict} 这个写法很显然只会比较键是不是完全一样，即使修改了形状也不会关心，所以需要加上判断，v.shape 和 model_state_dict[k].shape 要是一样的才行，这样的话，只会从预训练模型里面加载确定没有修改的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6af03667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([1000])\n",
      "Missing keys: ['fc.weight', 'fc.bias']\n",
      "Unexpected keys: []\n",
      "tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(0.0253, grad_fn=<StdBackward0>)\n",
      "tensor(5.8502e-08, grad_fn=<MeanBackward0>) tensor(0.0695, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = models.resnet18(weights=None) # 加载空模型\n",
    "\n",
    "model.fc = nn.Linear(512, 10) # 修改全连接层的大小\n",
    "\n",
    "pretrained_state_dict = resnet18.state_dict() # 预训练模型参数\n",
    "model_state_dict = model.state_dict() # 加载的模型参数\n",
    "\n",
    "# 验证键是一样的，也就是层没有改变\n",
    "print(pretrained_state_dict.keys() == model_state_dict.keys())\n",
    "print(pretrained_state_dict[\"fc.bias\"].size())\n",
    "\n",
    "pretrained_state_dict = {k : v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "# res = model.load_state_dict(torch.load('./resnet18_state_dict.pth'), strict=False) 报错，因为无法对应，原本的预训练模型的全连接层的大小为 [512, 1000]，现在被我修改为了 [512, 10]\n",
    "res = model.load_state_dict(pretrained_state_dict, strict=False) # 完美的加载，验证了自己的想法，现在删掉了全连接层的参数，其他的参数对应上了\n",
    "\n",
    "print(\"Missing keys:\", res.missing_keys) # 模型中存在，但是在预训练参数里面找不到\n",
    "print(\"Unexpected keys:\", res.unexpected_keys) # 预训练参数中存在模型中没定义，这个不会遇到，因为我强制的比对了键值对，模型中没定义的一定会被删掉的\n",
    "\n",
    "print(model.fc.weight.mean(), model.fc.weight.std())  # 随机分布\n",
    "print(resnet18.fc.weight.mean(), resnet18.fc.weight.std())  # 不同分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a32a2",
   "metadata": {},
   "source": [
    "> 上面简单的介绍了模型的加载和报错，和加载预训练参数的方法等，这个是 torchvision.models 库函数里面常见的操作了，总结下列函数：\n",
    "\n",
    "- 1）models.model() 加载名字为 model 的模型，一般的参数为 weights，代表加载预训练参数，不想加载的话初始化为 weights=None\n",
    "\n",
    "- 2）model.named_parameters() 返回一个生成器，每个元素是 (name, parameter) 二元组\n",
    "\n",
    "- 3）summary() 来自 torchsummary 或 torchinfo 包，需单独安装，能模拟一张输入图片走完整个网络，输出表格信息（每层输出尺寸、参数量、总参数数等）\n",
    "\n",
    "- 4）torch.save() 保存模型，可以保存任意 Python 对象包括模型参数、优化器状态、epoch 等\n",
    "\n",
    "- 5）torch.load_state_dict() 加载模型\n",
    "\n",
    "- 6）torch.load() 加载文件转化为字典\n",
    "\n",
    "- 7）model.state_dict() 返回全部参数加上缓冲区（不带梯度信息），named_parameters() 返回仅可训练参数（带梯度）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d10b5",
   "metadata": {},
   "source": [
    "##### 3. torchvision.datasets、torchvision.transforms 的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0648f",
   "metadata": {},
   "source": [
    "> - 2）torchvision.datasets：提供常用的数据集，设计上继承 torch.utils.data.Dataset，主要包含分类、检测、分割、视频理解四大类常见数据集：\n",
    "\n",
    "- - 2.1）分类方面主要包括：MNIST、FashionMNIST、CIFAR10/100、ImageNet、SVHN、Places365 等\n",
    "\n",
    "- - 2.2）检测、分割方面主要包含：VOC、COCO、Cityscapes、CelebA、Caltech101/256 等\n",
    "\n",
    "- - 2.3）视频理解方面不打算涉及，暂时不做学习\n",
    "\n",
    "> - 3）torchvision.transforms：提供常用的数据预处理操作，主要包括对 Tensor 及 PIL Image 对象的操作，新版的 transforms 有两套接口：\n",
    "\n",
    "- - 3.1）传统版 (transforms)：操作 PIL 图像或 Tensor\n",
    "\n",
    "- - 3.2）新版 (transforms.v2)：统一操作 tv_tensors（带标签的张量类型），支持批处理和自动类型推断，更安全且更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f521779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBILee6mENvDJNKwJCRqWY4GTwPYE0sttPAEM0MkYfO0upG7HBxnrUVFXdJ1a+0PVINT02cwXlu26KUKDtOMdCCDwTXVw/FvxerBrq9gvyGDD7ZbJJjC7cDjgHgnHUjPrnpfiBrpPw60uz1SOwbXtTcXkiW8aL9mg/5Z7TGcYYANhi2d2eCBjyKiu8+GHhy01LVLvXtY8n+xNDj+1XSy8iRhkohXvkg8Y5xjB6VzHiTXbnxJr11qlzgNK3yIudsaDhVUEnAA7dKyqK6lfiBrMXgVPCFutrBpwLGV44j5s+5i2HYkg8nsBwB6Vy1Ff/Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4AWJkZMANmHBLMQw+SRaIa1eePPPB68IOCAdOQv25x4GBgeHZKqjwCZEZEBYjGHQ8m/TM/fjfw3///v16+u9fSbAgI1QnWKFS8vRsBob3W9ap3NQGC0B1wlSC6BW/F6uCaEZGzODT0WZhvQPRiKmT+e/dSIhGRgzJ8s9/a6FymJLcf8u5YZLodgpN+PLvG9RGDDv//c2F6WNkhAYfTO30n6/YYGwGNJ2Aqe/+64vQiGonW4HTrl9IGlG84vv37xwkjShesT/49wUzLknuv38zkeWQ7ay6hWwdmI1Q+uXv3+2BCC4oNhG8L3/nQ6MKIQbWj50AAET1Q0O22Fn9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# root 数据集的存放路径，没有的话，会自动存放到该文件夹\n",
    "# train 是否加载训练集，否则的话就是测试集\n",
    "# transform torchvision.transforms 包含对图像进行变换的函数，ToTensor()、Normalize()\n",
    "# target_transform torchvision.transforms 对标签变换的函数，一般不使用\n",
    "# download 如果本地没有数据集，是否从网上下载\n",
    "mnist = datasets.MNIST(\"data/\", download=True, train=False, transform=transform)\n",
    "\n",
    "img = mnist[0][0]\n",
    "label = mnist[0][1]\n",
    "to_img = T.ToPILImage()\n",
    "display(to_img(img))\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2da528",
   "metadata": {},
   "source": [
    "- 1）当第一次调用 datasets.MNIST(download=True) 时，它会自动执行以下步骤检查 raw 文件夹如果没有 .gz 文件，就从官网（yann.lecun.com）下载，自动解压 .gz 文件\n",
    "\n",
    "- 2）存入 data/MNIST/processed/，解析二进制数据为 Tensor，把像素值解析成 torch.Tensor 格式，把标签解析成 torch.LongTensor，缓存结果，将解析好的数据保存为两个 .pt 文件\n",
    "\n",
    "- 3）training.pt 包含 (train_images, train_labels)，参考上面是输出 dataset 是一个存储文件，保存了 Tensor 类型的文件和对应的标签，test.pt 包含 (test_images, test_labels)\n",
    "\n",
    "- 4）因为 datasets.MNIST 继承自 torch.utils.data.Dataset，实现了 \\_\\_getitem\\_\\_ 和 \\_\\_len\\_\\_ 两个函数，可以直接 train_loader = DataLoader(train_data, batch_size=64, shuffle=True)，不需要也不能进行其他任何多余的操作，这个是 torchvision.datasets 的很大的局限性，只能加载指定名字的指定格式的数据集，想要将自己的数据集转化为 DataLoader 还是需要使用 torch.utils.data.Dataset 实现 \\_\\_getitem\\_\\_ 和 \\_\\_len\\_\\_ 函数，然后转化为 Dataset 类型的数据集，再进行 DataLoader 的批量加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5addbad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "0 torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "1 torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "2 torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "3 torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "4 torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "5 torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([10000, 28, 28]) torch.Size([10000])\n",
      "torch.Size([1, 28, 28]) 7\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(mnist, batch_size=64, shuffle=True)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "batch_img, batch_label = next(train_iter)\n",
    "\n",
    "print(batch_img.shape, batch_label.shape)\n",
    "for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "    print(batch_idx, imgs.shape, labels.shape)\n",
    "    if batch_idx == 5: break\n",
    "\n",
    "# train_data.data 输出所有图片，train_data.targets 输出所有的标签\n",
    "print(mnist.data.shape, mnist.targets.shape)\n",
    "\n",
    "image, label = mnist[0]\n",
    "print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5850cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AugMix',\n",
       " 'AutoAugment',\n",
       " 'AutoAugmentPolicy',\n",
       " 'CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'ElasticTransform',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'InterpolationMode',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandAugment',\n",
       " 'RandomAdjustSharpness',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomAutocontrast',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomEqualize',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomInvert',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomPosterize',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSolarize',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " 'TrivialAugmentWide',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional_pil',\n",
       " '_functional_tensor',\n",
       " '_presets',\n",
       " 'autoaugment',\n",
       " 'functional',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "# 包含了常见的图像处理的函数\n",
    "dir(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae4fc2",
   "metadata": {},
   "source": [
    "> 常见的图像处理函数的总结，用到可以自行查找\n",
    "\n",
    "- 1）Compose()：用来管理所有的 transforms 操作\n",
    "\n",
    "- 2）ToTensor()：把图片数据转换成张量并转化范围在 [0,1] 区间内\n",
    "\n",
    "- 3）Normalize(mean, std)：归一化\n",
    "\n",
    "- 4）Resize(size)：输入的 PIL 图像调整为指定的大小，参数可以为 int 或 int 元组\n",
    "\n",
    "- 5）CenterCrop(size)：将给定的 PIL Image 进行中心切割，得到指定 size 的 tuple\n",
    "\n",
    "- 6）RandomCrop(size, padding=0)：随机中心点切割\n",
    "\n",
    "- 7）RandomHorizontalFlip(size, interpolation=2)：将给定的 PIL Image 随机切割，再 resize\n",
    "\n",
    "- 8）RandomHorizontalFlip()：随机水平翻转给定的 PIL Image\n",
    "\n",
    "- 9）RandomVerticalFlip()：随机垂直翻转给定的 PIL Image\n",
    "\n",
    "- 10）ToPILImage()：将 Tensor 或 numpy.ndarray 转换为 PIL Image\n",
    "\n",
    "- 11）FiveCrop(size)：将给定的 PIL 图像裁剪成 4 个角落区域和中心区域\n",
    "\n",
    "- 12）Pad(padding, fill=0, padding_mode=‘constant’)：对 PIL 边缘进行填充\n",
    "\n",
    "- 13）RandomAffine(degrees, translate=None, scale=None)：保持中心不变的图片进行随机仿射变化\n",
    "\n",
    "- 15）RandomApply(transforms, p=0.5)：随机选取变换"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202510-learning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
