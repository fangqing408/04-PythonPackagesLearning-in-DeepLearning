{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003ac3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) 1\n",
      "torch.Size([3, 3]) 2\n",
      "torch.Size([1, 3]) 2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3]) # torch 的转化张量的方法为 torch.tensor()，转化为 numpy 数组的方法为 numpy.array()\n",
    "print(a.shape, a.ndim)\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(a.shape, a.ndim)\n",
    "\n",
    "a = torch.tensor([[1, 2, 3]])\n",
    "print(a.shape, a.ndim)\n",
    "\n",
    "print(len(a.shape) == a.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e535e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n",
      "tensor(30) torch.float64\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=float)\n",
    "print(b.numel(), math.prod(b.shape))  # torch 没有 size 属性，其中的 size() 是方法，shape 返回的就是这个方法，二者完全等价，numel() 方法等价于 numpy 是 size 属性\n",
    "print(torch.prod(torch.tensor((1, 2, 3, 5))), b.dtype) # torch.prod() 方法只接受参数为 tensor 的数据类型，numpy 接受 tuple 类型\n",
    "\n",
    "b = b.float()  # torch 与 numpy 转换元素数据类型所使用方法不同，前者使用 astype 方法，后者直接转化\n",
    "# 也可创建的时候直接指定类型 b = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=float)\n",
    "print(b, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9899daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.]) torch.Size([3])\n",
      "tensor([[1., 1., 1.]]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "c = torch.ones(3)\n",
    "print(c, c.shape)\n",
    "c = torch.ones((1, 3))\n",
    "print(c, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f6dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]]) torch.Size([2, 2, 3])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "d = [i for i in range(1, 13)]\n",
    "d = torch.tensor(d)\n",
    "\n",
    "d1 = d.reshape((2, 2, -1))\n",
    "print(d1, d1.shape)\n",
    "1\n",
    "d2 =  d.reshape((3, -1))\n",
    "print(d2, d2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a38c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) torch.Size([10])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) torch.Size([10])\n",
      "tensor([10, 12, 14, 16, 18, 20]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "e = torch.arange(10)\n",
    "print(e, e.shape)\n",
    "\n",
    "e = torch.arange(10, 20)\n",
    "print(e, e.shape)\n",
    "\n",
    "e = torch.arange(10, 21, 2)\n",
    "print(e, e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c16bb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) torch.Size([10])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) torch.Size([10])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "f = torch.ones(10)\n",
    "print(f, f.shape)\n",
    "\n",
    "f = torch.zeros(10)\n",
    "print(f, f.shape)\n",
    "\n",
    "f = torch.empty(10)\n",
    "print(f, f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54450d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.0000, 10.7143, 11.4286, 12.1429, 12.8571, 13.5714, 14.2857, 15.0000,\n",
      "        15.7143, 16.4286, 17.1429, 17.8571, 18.5714, 19.2857, 20.0000]) torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "g = torch.linspace(10, 20, 15)\n",
    "print(g, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da0ca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 1, 2, 4],\n",
      "        [2, 1, 5, 4],\n",
      "        [8, 7, 9, 6],\n",
      "        [9, 3, 4, 1],\n",
      "        [1, 3, 8, 2]]) torch.Size([5, 4])\n",
      "tensor([[1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 8, 9, 9]])\n",
      "tensor([[1, 1, 2, 1],\n",
      "        [2, 1, 4, 2],\n",
      "        [3, 3, 5, 4],\n",
      "        [8, 3, 8, 4],\n",
      "        [9, 7, 9, 6]])\n",
      "tensor([[3, 1, 2, 4, 2],\n",
      "        [1, 5, 4, 8, 7]]) torch.Size([2, 5])\n",
      "tensor([[1, 2, 2, 3, 4],\n",
      "        [1, 4, 5, 7, 8]]) torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "h = torch.tensor([[3, 1, 2, 4], [2, 1, 5, 4], [8, 7, 9, 6], [9, 3, 4, 1], [1, 3, 8, 2]])\n",
    "print(h, h.shape)\n",
    "\n",
    "h1 = h.reshape((1, -1))\n",
    "h1, _ = torch.sort(h1) # torch 返回值是一个 tuple，(value, indices)，代表排序后的值和排序前的下标\n",
    "print(h1)\n",
    "\n",
    "h2 = h\n",
    "h2, _ = torch.sort(h2, dim=0) # torch 轴参数一般写为 dim，numpy 写为 axis，默认为 -1 按照最后一个维度排序\n",
    "print(h2)\n",
    "\n",
    "h3, _ = h.reshape((2, 2, -1))\n",
    "print(h3, h3.shape)\n",
    "\n",
    "h4 = h3\n",
    "h4, _ = torch.sort(h4, dim=-1)\n",
    "print(h4, h4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3259b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]]) torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor([[1, 2], [3, 4]])\n",
    "j = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "i = torch.cat((i, j), dim=-1) # torch 轴参数为 dim，numpy 轴参数为 axis，torch 的 cat() 方法与 numpy 的 concatenate() 方法有一样的效果\n",
    "print(i, i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98096729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([61.2704, 97.2332, 78.3140, 91.9332, 95.4287, 87.9820, 98.7736, 90.9288,\n",
      "        78.8261, 62.9807]) torch.Size([10])\n",
      "tensor([[2, 4, 7],\n",
      "        [8, 4, 9],\n",
      "        [8, 1, 3]]) torch.Size([3, 3])\n",
      "tensor([[ 1.6986, -0.6187,  0.3423],\n",
      "        [ 0.7636, -0.4389, -0.1773]]) torch.Size([2, 3])\n",
      "tensor([[ 0.7433, -0.2108,  0.5234],\n",
      "        [ 0.1672,  0.6574,  0.0608]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch 的 randint() 和 normal() 方法都不接受向量类型的参数，也就是一维的 numpy 数组\n",
    "k = 40 * torch.rand(10) + 60 # torch 的 rand() 方法与 numpy 的 random.random() 方法有一样的效果\n",
    "print(k, k.shape)\n",
    "\n",
    "k = torch.randint(1, 10, (3, 3)) # torch 的 randint() 方法与 numpy 的 random.randint() 方法有一样的效果\n",
    "print(k, k.shape)\n",
    "\n",
    "k = torch.normal(0, 1, (2, 3)) # torch 的 normal() 方法与 numpy 的 random.normal() 方法有一样的效果\n",
    "print(k, k.shape)\n",
    "\n",
    "k = torch.randn(2, 3) # torch 的 randn() 方法与 numpy 的 random.randn() 方法有一样的效果\n",
    "print(k, k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f4770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 9]) torch.Size([3])\n",
      "tensor([[100,   1,   2,   4],\n",
      "        [  2, 100,   5,   4],\n",
      "        [  8,   7, 100,   6],\n",
      "        [  9,   3,   4,   1],\n",
      "        [  1,   3,   8,   2]]) torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[3, 1, 2, 4], [2, 1, 5, 4], [8, 7, 9, 6], [9, 3, 4, 1], [1, 3, 8, 2]])\n",
    "m1 = m[[0, 1, 2], [0, 1, 2]]\n",
    "print(m1, m1.shape)\n",
    "\n",
    "m[[0, 1, 2], [0, 1, 2]] = 100\n",
    "print(m, m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db24f33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]) torch.Size([10])\n",
      "tensor([3, 4, 5, 6, 7, 8]) torch.Size([6])\n",
      "tensor([1, 3, 5, 7, 9]) torch.Size([5])\n",
      "tensor([2, 4, 6, 8]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "n = torch.arange(1, 11)\n",
    "print(n, n.shape)\n",
    "\n",
    "n1 = n[2:-2]\n",
    "print(n1, n1.shape)\n",
    "\n",
    "n2 = n[::2]\n",
    "print(n2, n2.shape)\n",
    "\n",
    "n3 = n[1:-1:2]\n",
    "print(n3, n3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f2809c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3],\n",
      "        [7, 9]])\n",
      "tensor([7, 8, 9]) tensor([7, 8, 9])\n",
      "tensor([[3],\n",
      "        [6],\n",
      "        [9]])\n",
      "tensor([[2, 3],\n",
      "        [5, 6],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "o = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(o[::2, ::2])\n",
    "\n",
    "print(o[2], o[2, :])\n",
    "\n",
    "print(o[:, 2].reshape(1, -1).T) # 直接输出 o[:, 2] 是一个向量不是列矩阵，采用本方法转化，截取列大于 1，就会自动转化为列矩阵，等价于 o[:, 2].reshape(-1, 1)\n",
    "print(o[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aae5ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1, 100,   3,   4,   5,   6,   7,   8,   9,  10]) torch.Size([10])\n",
      "tensor([  1, 100,   3,   4,   5,   6,   7,   8,   9,  10]) torch.Size([10])\n",
      "tensor([1000,  100,    3,    4,    5,    6,    7,    8,    9,   10]) torch.Size([10])\n",
      "tensor([1000,  100,    3,    4,    5,    6,    7,    8,    9,   10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "p = torch.arange(1, 11)\n",
    "p1 = p[1:5]\n",
    "p1[0] = 100\n",
    "print(p, p.shape)\n",
    "\n",
    "p2 = p[1:5].clone() # torch 的 clone() 方法和 numpy 的 copy() 方法有相同的效果\n",
    "p2[0] = 1000\n",
    "print(p, p.shape)\n",
    "\n",
    "p3 = p\n",
    "p3[0] = 1000\n",
    "print(p, p.shape)\n",
    "\n",
    "p4 = p.clone()\n",
    "p4[0] = 10000\n",
    "print(p, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "859e236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  9,  8,  7,  6,  5,  4,  3,  2,  1]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "q = torch.arange(1, 11)\n",
    "q = torch.flipud(q)\n",
    "print(q, q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a5f0a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 8, 9],\n",
      "        [4, 5, 6],\n",
      "        [1, 2, 3]]) torch.Size([3, 3])\n",
      "tensor([[3, 2, 1],\n",
      "        [6, 5, 4],\n",
      "        [9, 8, 7]]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "r = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "rud = torch.flipud(r)\n",
    "rlr = torch.fliplr(r)\n",
    "print(rud, rud.shape)\n",
    "print(rlr, rlr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5364ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2]) tensor([3, 4, 5, 6, 7, 8]) tensor([ 9, 10])\n",
      "tensor([100,   2,   3,   4,   5,   6,   7,   8,   9,  10])\n",
      "tensor([[100,   2,   3,   4,   5]]) tensor([[ 6,  7,  8,  9, 10]])\n",
      "tensor([[100],\n",
      "        [  6]])\n",
      "tensor([[ 2,  3,  4,  5],\n",
      "        [ 7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "s = torch.arange(1, 11)\n",
    "\n",
    "s1, s2, s3 = torch.split(s, [2, 6, 2]) # torch 的 split() 方法用起来和 numpy 的完全不一样，前者需要手动的指定块的大小，如果是一个值的话代表等分，后者代表切割的位置\n",
    "# 块的大小的加和要严格的相等，需要手动算一下，不能等于吧最后一个维度留作 -1， 除非等分的时候，最后不足才会自动切割\n",
    "print(s1, s2, s3)\n",
    "s1[0] = 100\n",
    "print(s)\n",
    "\n",
    "s = s.reshape((2, -1))\n",
    "s1, s2 = torch.split(s, [1, 1], dim=0)\n",
    "print(s1, s2)\n",
    "\n",
    "s1, s2 = torch.split(s, [1, 4], dim=1)\n",
    "print(s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebbb510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  8, 27, 64])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(1, 5)\n",
    "print(t ** 3)\n",
    "\n",
    "t1 = -t\n",
    "print(t)\n",
    "t1[0] = 100\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f4f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([0, 1, 2])\n",
      "tensor([[ 0,  1,  4,  9, 16],\n",
      "        [ 0,  6, 14, 24, 36],\n",
      "        [ 0, 11, 24, 39, 56]])\n",
      "tensor([[ 0,  0,  0,  0,  0],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [20, 22, 24, 26, 28]])\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 2, 4, 6, 8]])\n"
     ]
    }
   ],
   "source": [
    "u1 = torch.arange(0, 5)\n",
    "u2 = torch.arange(0, 15).reshape(3, -1)\n",
    "u3 = torch.arange(0, 3)\n",
    "\n",
    "print(u1)\n",
    "print(u2)\n",
    "print(u3)\n",
    "# 广播和矩阵乘法没有任何关系，广播成 (x, y) 大小，逐元素相乘而已，左乘本身要是 (1, y)，右乘本身要是 (x, 1)\n",
    "\n",
    "print(u1 * u2)\n",
    "print(u3.reshape(-1, 1) * u2)\n",
    "\n",
    "# 当形状为 (1, y) 和 (x, 1) 相乘，都广播为大小 (x, y) 逐元素相乘\n",
    "\n",
    "print(u1 * u3.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "349a56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30)\n",
      "tensor([ 90, 100, 110])\n",
      "tensor([[  8,   9,  10,  11,  12,  13,  14,  15],\n",
      "        [ 24,  29,  34,  39,  44,  49,  54,  59],\n",
      "        [ 40,  49,  58,  67,  76,  85,  94, 103],\n",
      "        [ 56,  69,  82,  95, 108, 121, 134, 147],\n",
      "        [ 72,  89, 106, 123, 140, 157, 174, 191]])\n"
     ]
    }
   ],
   "source": [
    "v1 = torch.arange(0, 5)\n",
    "v2 = torch.arange(0, 5)\n",
    "\n",
    "print(torch.dot(v1, v2)) # 矩阵乘法混有向量的时候，根据需求自动的充当行矩阵或者列矩阵，混有向量的时候输出结果一定是向量\n",
    "# torch 使用 dot 的时候，必须保证两个全是向量\n",
    "\n",
    "v3 = torch.arange(0, 15).reshape(5, -1)\n",
    "print(torch.matmul(v1, v3)) # torch 向量乘向量的时候可以使用 dot() 方法，但是只要两者之间出现矩阵，就必须使用 matmul() 方法，当然向量之间的乘法也可以使用 matmul() 方法\n",
    "\n",
    "# 除此之外，向量和矩阵的乘积可以使用 mv() 方法，矩阵和矩阵的乘积可以使用 mm() 方法，matmul() 方法是万能的\n",
    "\n",
    "v4 = torch.arange(0, 10).reshape(5, -1)\n",
    "v5 = torch.arange(0, 16).reshape(2, -1)\n",
    "\n",
    "print(torch.mm(v4, v5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69f6e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([ 0.8415,  0.9093, -0.1411]) tensor([ 0.5403, -0.4161, -0.9900]) tensor([ 1.5574, -2.1850,  0.1425])\n",
      "tensor([ 2.7183,  7.3891, 20.0855]) tensor([2, 4, 8]) tensor([  10,  100, 1000])\n",
      "tensor([0.0000, 2.3026, 4.6052, 6.9078]) tensor([0.0000, 3.3219, 6.6439, 9.9658]) tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1, 2, -3])\n",
    "\n",
    "print(torch.abs(w))\n",
    "print(torch.sin(w), torch.cos(w), torch.tan(w))\n",
    "\n",
    "w1 = torch.tensor([1, 2, 3])\n",
    "print(torch.exp(w1), 2 ** w1, 10 ** w1) # torch 的 exp() 函数必须传入张量类型 \n",
    "\n",
    "w2 = torch.tensor([1, 10, 100, 1000])\n",
    "print(torch.log(w2), torch.log(w2) / torch.log(torch.tensor(2)), torch.log(w2) / torch.log(torch.tensor(10)))\n",
    "# torch 的 log() 函数必须传入张量类型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49dc7416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.]) tensor([3., 6.])\n",
      "tensor([5., 7., 9.]) tensor([ 6., 15.])\n",
      "tensor([ 4., 10., 18.]) tensor([  6., 120.])\n",
      "tensor([2.5000, 3.5000, 4.5000]) tensor([2.1213, 2.1213, 2.1213])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]]).float()\n",
    "\n",
    "print(torch.max(x, dim=0)[0], torch.max(x, dim=1)[0]) # torch 的 max 函数也会返回最大值和最大值的位置\n",
    "print(torch.sum(x, dim=0), torch.sum(x, dim=1))\n",
    "print(torch.prod(x, dim=0), torch.prod(x, dim=1))\n",
    "print(torch.mean(x, dim=0), torch.std(x, dim=0)) # torch 里面的 mean()、std() 函数只接受浮点类型的数组\n",
    "\n",
    "# nanmax ... 忽略缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12f3e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6822)\n",
      "tensor([False, False,  True]) tensor(True) tensor(False)\n"
     ]
    }
   ],
   "source": [
    "y = torch.normal(0, 1, (10000, ))\n",
    "\n",
    "print(torch.sum(torch.abs(y) <= 1))\n",
    "\n",
    "y1 = torch.tensor([1, 2, 3])\n",
    "y2 = torch.tensor([3, 4, 3])\n",
    "print(y1 == y2, torch.any(y1 == y2), torch.all(y1 == y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27b0666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 7, 8, 9]) (tensor([1, 1, 1, 2, 2, 2]), tensor([0, 1, 2, 0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "z1 = z >= 4\n",
    "print(z[z1], torch.where(z1)) # 矩阵进行掩码操作变化为了向量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202510-learning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
