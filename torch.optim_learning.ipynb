{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea29e2c7",
   "metadata": {},
   "source": [
    "requires_grad=True PyTorch 会记录它的计算图并在 .grad 中保存梯度\n",
    "\n",
    "loss.backward() 所有可训练参数的 .grad 被填充\n",
    "\n",
    "optimizer.step() 对应的是 param.data -= lr * param.grad 手动更新权重这句话\n",
    "\n",
    "requires_grad=False 冻结这层参数（不求导，不更新）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580d0437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight True tensor([[-0.0760, -0.1521],\n",
      "        [-0.2768, -0.5536]]) tensor([[0.1295, 0.1109],\n",
      "        [0.6487, 0.4996]])\n",
      "fc1.bias True tensor([-0.0760, -0.2768]) tensor([-0.0975, -0.2738])\n",
      "fc2.weight False None tensor([[0.4788, 0.3162],\n",
      "        [0.1909, 0.6951]])\n",
      "fc2.bias False None tensor([-0.5581,  0.3771])\n",
      "fc3.weight True tensor([[ 0.0000, -3.2985]]) tensor([[0.6921, 0.1667]])\n",
      "fc3.bias True tensor([-2.3890]) tensor([-0.4246])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # layer1\n",
    "        x = F.relu(self.fc2(x))   # layer2\n",
    "        x = self.fc3(x)           # layer3\n",
    "        return x\n",
    "\n",
    "net = TinyNet()\n",
    "\n",
    "# 冻结中间层 fc2\n",
    "for p in net.fc2.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 输入与目标\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "y_true = torch.tensor([[1.0]])\n",
    "\n",
    "# 前向\n",
    "y_pred = net(x)\n",
    "loss = F.mse_loss(y_pred, y_true)\n",
    "loss.backward()\n",
    "\n",
    "# 查看梯度\n",
    "for name, p in net.named_parameters():\n",
    "    print(name, p.requires_grad, p.grad, p.data) # 可以发现梯度还是会向前传播的，也就是冻结的层也会参与链式法则的计算，只是自己不更新梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5babb2e",
   "metadata": {},
   "source": [
    "torch.optim 是整个系统里面唯一修改参数值的模块，接收可训练参数（requires_grad=True）根据梯度 .grad 计算更新规则并修改参数 .data，也就是 param.data -= lr * param.grad 的高级封装\n",
    "\n",
    "所有的优化器都继承自 torch.optim.Optimizer 每个优化器都有这三类成员：\n",
    "\n",
    "params：要更新的参数（通常是 model.parameters()）\n",
    "\n",
    "state：每个参数的历史状态（比如动量、平方梯度等）\n",
    "\n",
    "param_groups：一组参数的配置字典（比如不同层不同学习率）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd131dde",
   "metadata": {},
   "source": [
    "常见的优化器一共可分为两大类，分为无动量类和包含动量的类：SGD、Adagrad、Adam、AdamW、Adamax、SGD + momentum 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有优化器都有这些基础参数\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,          # 学习率\n",
    "    betas=(0.9, 0.999),  # Adam 特有\n",
    "    eps=1e-8,\n",
    "    weight_decay=1e-4,   # 权重衰减（L2正则）\n",
    ")\n",
    "\n",
    "# 可以给不同层不同的学习率\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.backbone.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.head.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "\n",
    "# 完整的训练过程下，优化器一般的使用如下\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()   # 1️⃣ 清空上次的梯度\n",
    "    y_pred = model(x)       # 2️⃣ 前向传播\n",
    "    loss = criterion(y_pred, y)  # 3️⃣ 计算损失\n",
    "    loss.backward()         # 4️⃣ 反向传播（autograd 填充梯度）\n",
    "    optimizer.step()        # 5️⃣ 参数更新\n",
    "\n",
    "# .zero_grad() 必须放在 .backward() 前\n",
    "\n",
    "# .step() 必须放在 .backward() 后"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f7a92",
   "metadata": {},
   "source": [
    "在 torch.optim.lr_scheduler 子模块中，可以动态修改学习率（LR Scheduling）\n",
    "\n",
    "StepLR：每隔几步衰减 LR\n",
    "\n",
    "MultiStepLR：在指定 epoch 衰减 LR\n",
    "\n",
    "ExponentialLR：按指数衰减\n",
    "\n",
    "CosineAnnealingLR：余弦退火\n",
    "\n",
    "ReduceLROnPlateau：当验证集 loss 不再下降时自动降 LR\n",
    "\n",
    "OneCycleLR：Transformer 常用，先升后降\n",
    "\n",
    "get_lr()：获取当前学习率\n",
    "\n",
    "step()：执行一次学习率更新（通常每个 epoch 调一次）\n",
    "\n",
    "state_dict() / load_state_dict()：保存 / 恢复状态\n",
    "\n",
    "last_epoch：当前步数或 epoch 计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0503fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "for epoch in range(30):\n",
    "    train(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8fbd7",
   "metadata": {},
   "source": [
    "weight_decay 是优化器参数 L2 正则项，防止过拟合\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_ 独立函数，限制梯度范数（防梯度爆炸）\n",
    "\n",
    "torch.nn.utils.clip_grad_value_\t独立函数，限制梯度值范围\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "所以优化器“自动忽略冻结层”，因为它只看到那些 requires_grad=True 的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的做法可以在训练中断的时候保存当前的模型参数和优化器\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, 'checkpoint.pth')\n",
    "\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e724bd",
   "metadata": {},
   "source": [
    "zero_grad() 清空梯度（通常每个 batch 前调用），step() 执行一次参数更新（核心）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202510-learning (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
